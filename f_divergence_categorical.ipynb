{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.get_lock().locks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_categorical(dim):\n",
    "    return torch.distributions.categorical.Categorical(logits=torch.randn(dim)).probs\n",
    "\n",
    "dim = 64\n",
    "nu = random_categorical(dim)\n",
    "mu = random_categorical(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gamma_KL(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def init_gamma(f, nu):\n",
    "        return torch.mean(f)\n",
    "        \n",
    "    @staticmethod\n",
    "    def F(f, nu, gamma):\n",
    "        return -torch.sum(nu * torch.exp(f - gamma), dim=-1) + 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def F_derivative(f, nu, gamma):\n",
    "        return torch.sum(nu * torch.exp(f - gamma))\n",
    "    \n",
    "    @staticmethod\n",
    "    def F_gradient(f, nu, gamma):\n",
    "        return -nu * torch.exp(f - gamma)\n",
    "    \n",
    "    @staticmethod\n",
    "    def phi(x):\n",
    "        return x * torch.log(x) - x + 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def phi_conjugate(y):\n",
    "        return torch.exp(y) - 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, f, nu):\n",
    "        f, nu = f.detach(), nu.detach()\n",
    "        \n",
    "        tol = 1e-6\n",
    "        max_steps = 10000\n",
    "\n",
    "        gamma = Gamma_KL.init_gamma(f, nu)\n",
    "        for i in range(max_steps):\n",
    "            gamma_prev = gamma\n",
    "            F = Gamma_KL.F(f, nu, gamma)\n",
    "            F_derivative = Gamma_KL.F_derivative(f, nu, gamma)\n",
    "            gamma = gamma_prev - F / F_derivative\n",
    "            if torch.abs(gamma - gamma_prev) < tol:\n",
    "                break\n",
    "        if torch.abs(gamma - gamma_prev) >= tol:\n",
    "            print(f\"Newton: tolerance not reached\")\n",
    "\n",
    "        ctx.save_for_backward(f, nu, gamma)\n",
    "\n",
    "        return gamma\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        with torch.no_grad():\n",
    "            f, nu, gamma = ctx.saved_tensors\n",
    "            grad_input_f = -Gamma_KL.F_gradient(f, nu, gamma) / Gamma_KL.F_derivative(f, nu, gamma)\n",
    "        return grad_input_f * grad_output, None\n",
    "    \n",
    "    @staticmethod\n",
    "    def conjugate(f, nu):\n",
    "        gamma = Gamma_KL.apply(f, nu)\n",
    "        return torch.sum(nu * Gamma_KL.phi_conjugate(f - gamma), dim=-1) + gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL exact: 0.72302\n"
     ]
    }
   ],
   "source": [
    "print(f\"KL exact: {torch.sum(nu * Gamma_KL.phi(mu / nu)).item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cdfdca42e17490887f7dd4ad00fe6ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=2000.0), HTML(value='')), layout=Layout(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KL approx: 0.72302\n"
     ]
    }
   ],
   "source": [
    "iterations = 2000\n",
    "f = torch.zeros(dim, requires_grad=True)\n",
    "optimizer = torch.optim.SGD((f,), lr=1e-1, momentum=0.9)\n",
    "t = tqdm(range(iterations), ncols='100%')\n",
    "for _ in t:\n",
    "    optimizer.zero_grad()\n",
    "    objective = torch.sum(mu * f, dim=-1) - Gamma_KL.conjugate(f, nu)\n",
    "    loss = -objective\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    t.set_description(f\"objective: {objective.item():.5f}\")\n",
    "\n",
    "print(f\"KL approx: {(torch.sum(mu * f, dim=-1) - Gamma_KL.conjugate(f, nu)).item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb57c4162dd54b0c8ae208c7c5d87200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=2000.0), HTML(value='')), layout=Layout(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KL approx closed form: 0.72302\n"
     ]
    }
   ],
   "source": [
    "iterations = 2000\n",
    "f = torch.zeros(dim, requires_grad=True)\n",
    "optimizer = torch.optim.SGD((f,), lr=1e-1, momentum=0.9)\n",
    "t = tqdm(range(iterations), ncols='100%')\n",
    "for _ in t:\n",
    "    optimizer.zero_grad()\n",
    "    objective = torch.sum(mu * f, dim=-1) - torch.log(torch.sum(nu * torch.exp(f)))\n",
    "    loss = -objective\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    t.set_description(f\"objective: {objective.item():.5f}\")\n",
    "\n",
    "print(f\"KL approx closed form: {(torch.sum(mu * f, dim=-1) - torch.log(torch.sum(nu * torch.exp(f)))).item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gamma_reverseKL(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def init_gamma(f, nu):\n",
    "        return torch.max(f) - 1 + 0.01\n",
    "        \n",
    "    @staticmethod\n",
    "    def F(f, nu, gamma):\n",
    "        return -torch.sum(nu * (1 / (1 - f + gamma)), dim=-1) + 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def F_derivative(f, nu, gamma):\n",
    "        return torch.sum(nu * (1 / ((1 - f + gamma) ** 2)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def F_gradient(f, nu, gamma):\n",
    "        return -nu * (1 / ((1 - f + gamma) ** 2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def phi(x):\n",
    "        return x - 1 - torch.log(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def phi_conjugate(y):\n",
    "        return -torch.log(1 - y)\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, f, nu):\n",
    "        f, nu = f.detach(), nu.detach()\n",
    "        \n",
    "        tol = 1e-6\n",
    "        max_steps = 10000\n",
    "\n",
    "        gamma = Gamma_reverseKL.init_gamma(f, nu)\n",
    "        for i in range(max_steps):\n",
    "            gamma_prev = gamma\n",
    "            F = Gamma_reverseKL.F(f, nu, gamma)\n",
    "            F_derivative = Gamma_reverseKL.F_derivative(f, nu, gamma)\n",
    "            gamma = gamma_prev - F / F_derivative\n",
    "            if torch.abs(gamma - gamma_prev) < tol:\n",
    "                break\n",
    "        if torch.abs(gamma - gamma_prev) >= tol:\n",
    "            print(f\"Newton: tolerance not reached\")\n",
    "        \n",
    "        ctx.save_for_backward(f, nu, gamma)\n",
    "\n",
    "        return gamma\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        with torch.no_grad():\n",
    "            f, nu, gamma = ctx.saved_tensors\n",
    "            grad_input_f = -Gamma_reverseKL.F_gradient(f, nu, gamma) / Gamma_reverseKL.F_derivative(f, nu, gamma)\n",
    "        return grad_input_f * grad_output, None\n",
    "    \n",
    "    @staticmethod\n",
    "    def conjugate(f, nu):\n",
    "        gamma = Gamma_reverseKL.apply(f, nu)\n",
    "        return torch.sum(nu * Gamma_reverseKL.phi_conjugate(f - gamma), dim=-1) + gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reverse KL exact: 0.64295\n"
     ]
    }
   ],
   "source": [
    "print(f\"reverse KL exact: {torch.sum(nu * Gamma_reverseKL.phi(mu / nu)).item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27e9068084f4ac783dedf63e674aa65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=10000.0), HTML(value='')), layout=Layout(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "reverse KL approx: 0.64273\n"
     ]
    }
   ],
   "source": [
    "iterations = 10000\n",
    "f = torch.zeros(dim, requires_grad=True)\n",
    "optimizer = torch.optim.SGD((f,), lr=1e-1, momentum=0.9)\n",
    "t = tqdm(range(iterations), ncols='100%')\n",
    "for _ in t:\n",
    "    optimizer.zero_grad()\n",
    "    objective = torch.sum(mu * f, dim=-1) - (Gamma_reverseKL.conjugate(f - torch.max(f), nu) + torch.max(f))\n",
    "    loss = -objective\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    t.set_description(f\"objective: {objective.item():.5f}\")\n",
    "\n",
    "print(f\"reverse KL approx: {(torch.sum(mu * f, dim=-1) - Gamma_reverseKL.conjugate(f, nu)).item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gamma_Chi2(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def init_gamma(f, nu):\n",
    "        return torch.max(f)\n",
    "        \n",
    "    @staticmethod\n",
    "    def F(f, nu, gamma):\n",
    "        return -torch.sum(nu * ((1 / 2) * (f - gamma) + 1), dim=-1) + 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def F_derivative(f, nu, gamma):\n",
    "        return 1 / 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def F_gradient(f, nu, gamma):\n",
    "        return -nu * (1 / 2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def phi(x):\n",
    "        return (x - 1) ** 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def phi_conjugate(y):\n",
    "        return (1 / 4) * (y ** 2) + y\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, f, nu):\n",
    "        f, nu = f.detach(), nu.detach()\n",
    "        \n",
    "        tol = 1e-5\n",
    "        max_steps = 10000\n",
    "\n",
    "        gamma = Gamma_Chi2.init_gamma(f, nu)\n",
    "        for i in range(max_steps):\n",
    "            gamma_prev = gamma\n",
    "            F = Gamma_Chi2.F(f, nu, gamma)\n",
    "            F_derivative = Gamma_Chi2.F_derivative(f, nu, gamma)\n",
    "            gamma = gamma_prev - F / F_derivative\n",
    "            if torch.abs(gamma - gamma_prev) < tol:\n",
    "                break\n",
    "        if torch.abs(gamma - gamma_prev) >= tol:\n",
    "            print(f\"Newton: tolerance not reached\")\n",
    "        \n",
    "        ctx.save_for_backward(f, nu, gamma)\n",
    "\n",
    "        return gamma\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        with torch.no_grad():\n",
    "            f, nu, gamma = ctx.saved_tensors\n",
    "            grad_input_f = -Gamma_Chi2.F_gradient(f, nu, gamma) / Gamma_Chi2.F_derivative(f, nu, gamma)\n",
    "        return grad_input_f * grad_output, None\n",
    "    \n",
    "    @staticmethod\n",
    "    def conjugate(f, nu):\n",
    "        gamma = Gamma_Chi2.apply(f, nu)\n",
    "        return torch.sum(nu * Gamma_Chi2.phi_conjugate(f - gamma), dim=-1) + gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi2 exact: 2.64669\n"
     ]
    }
   ],
   "source": [
    "print(f\"Chi2 exact: {torch.sum(nu * Gamma_Chi2.phi(mu / nu)).item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cfad9c730be422a9d15b017cb6b14e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=10000.0), HTML(value='')), layout=Layout(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chi2 approx: 2.64669\n"
     ]
    }
   ],
   "source": [
    "iterations = 10000\n",
    "f = torch.zeros(dim, requires_grad=True)\n",
    "optimizer = torch.optim.SGD((f,), lr=1e-1, momentum=0.9)\n",
    "t = tqdm(range(iterations), ncols='100%')\n",
    "for _ in t:\n",
    "    optimizer.zero_grad()\n",
    "    objective = torch.sum(mu * f, dim=-1) - (Gamma_Chi2.conjugate(f - torch.max(f), nu) + torch.max(f))\n",
    "    loss = -objective\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    t.set_description(f\"objective: {objective.item():.5f}\")\n",
    "\n",
    "print(f\"Chi2 approx: {(torch.sum(mu * f, dim=-1) - Gamma_Chi2.conjugate(f, nu)).item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6888f0323b45149f400efd5533c206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=10000.0), HTML(value='')), layout=Layout(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chi2 approx closed form: 2.64669\n"
     ]
    }
   ],
   "source": [
    "iterations = 10000\n",
    "f = torch.zeros(dim, requires_grad=True)\n",
    "optimizer = torch.optim.SGD((f,), lr=1e-1, momentum=0.9)\n",
    "t = tqdm(range(iterations), ncols='100%')\n",
    "for _ in t:\n",
    "    optimizer.zero_grad()\n",
    "    objective = torch.sum(mu * f, dim=-1) - (torch.sum(nu * f, dim=-1) + (1 / 4) * torch.sum(nu * ((f - torch.sum(nu * f, dim=-1)) ** 2), dim=-1))\n",
    "    loss = -objective\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    t.set_description(f\"objective: {objective.item():.5f}\")\n",
    "\n",
    "print(f\"Chi2 approx closed form: {(torch.sum(mu * f, dim=-1) - (torch.sum(nu * f, dim=-1) + (1 / 4) * torch.sum(nu * ((f - torch.sum(nu * f, dim=-1)) ** 2), dim=-1))).item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gamma_reverseChi2(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def init_gamma(f, nu):\n",
    "        return torch.max(f) - 1 + 1e-7\n",
    "        \n",
    "    @staticmethod\n",
    "    def F(f, nu, gamma):\n",
    "        return -torch.sum(nu * (1 / torch.sqrt(1 - f + gamma)), dim=-1) + 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def F_derivative(f, nu, gamma):\n",
    "        return torch.sum(nu * (1 / (2 * (torch.sqrt(1 - f + gamma) ** 3))))\n",
    "    \n",
    "    @staticmethod\n",
    "    def F_gradient(f, nu, gamma):\n",
    "        return -nu * (1 / (2 * (torch.sqrt(1 - f + gamma) ** 3)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def phi(x):\n",
    "        return 1 / x + x - 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def phi_conjugate(y):\n",
    "        return 2 - 2 * torch.sqrt(1 - y)\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, f, nu):\n",
    "        with torch.no_grad():\n",
    "            f, nu = f.detach(), nu.detach()\n",
    "\n",
    "            tol = 1e-6\n",
    "            max_steps = 10000\n",
    "\n",
    "            gamma = Gamma_reverseChi2.init_gamma(f, nu)\n",
    "            for i in range(max_steps):\n",
    "                gamma_prev = gamma\n",
    "                F = Gamma_reverseChi2.F(f, nu, gamma)\n",
    "                F_derivative = Gamma_reverseChi2.F_derivative(f, nu, gamma)\n",
    "                gamma = gamma_prev - F / F_derivative\n",
    "                if torch.abs(gamma - gamma_prev) < tol or Gamma_reverseChi2.F(f, nu, gamma) == 0.0:\n",
    "                    break\n",
    "            if torch.abs(gamma - gamma_prev) >= tol:\n",
    "                print(f\"Newton: tolerance not reached\")\n",
    "        \n",
    "        ctx.save_for_backward(f, nu, gamma)\n",
    "\n",
    "        return gamma\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        with torch.no_grad():\n",
    "            f, nu, gamma = ctx.saved_tensors\n",
    "            grad_input_f = -Gamma_reverseChi2.F_gradient(f, nu, gamma) / Gamma_reverseChi2.F_derivative(f, nu, gamma)\n",
    "        return grad_input_f * grad_output, None\n",
    "    \n",
    "    @staticmethod\n",
    "    def conjugate(f, nu):\n",
    "        gamma = Gamma_reverseChi2.apply(f, nu)\n",
    "        return torch.sum(nu * Gamma_reverseChi2.phi_conjugate(f - gamma), dim=-1) + gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reverseChi2 exact: 1.78507\n"
     ]
    }
   ],
   "source": [
    "print(f\"reverseChi2 exact: {torch.sum(nu * Gamma_reverseChi2.phi(mu / nu)).item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0bdd840b59d49ce974731e19fb9e7e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=1000000.0), HTML(value='')), layout=Layou…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "reverseChi2 approx: 1.78482\n"
     ]
    }
   ],
   "source": [
    "iterations = 1000000\n",
    "f = torch.zeros(dim, requires_grad=True)\n",
    "optimizer = torch.optim.SGD((f,), lr=1e-1, momentum=0.9)\n",
    "t = tqdm(range(iterations), ncols='100%')\n",
    "for _ in t:\n",
    "    optimizer.zero_grad()\n",
    "    objective = torch.sum(mu * f, dim=-1) - (Gamma_reverseChi2.conjugate(f - torch.max(f), nu) + torch.max(f))\n",
    "    loss = -objective\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    t.set_description(f\"objective: {objective.item():.5f}\")\n",
    "\n",
    "print(f\"reverseChi2 approx: {(torch.sum(mu * f, dim=-1) - (Gamma_reverseChi2.conjugate(f - torch.max(f), nu) + torch.max(f))).item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gamma_Hellinger2(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def init_gamma(f, nu):\n",
    "        return torch.max(f) - 1 + 0.1\n",
    "        \n",
    "    @staticmethod\n",
    "    def F(f, nu, gamma):\n",
    "        return -torch.sum(nu * (1 / ((1 - f + gamma) ** 2)), dim=-1) + 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def F_derivative(f, nu, gamma):\n",
    "        return torch.sum(nu * (2 / ((1 - f + gamma) ** 3)), dim=-1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def F_gradient(f, nu, gamma):\n",
    "        return -nu * (2 / ((1 - f + gamma) ** 3))\n",
    "    \n",
    "    @staticmethod\n",
    "    def phi(x):\n",
    "        return (x ** (1/2) - 1) ** 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def phi_conjugate(y):\n",
    "        return y / (1 - y)\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, f, nu):\n",
    "        f, nu = f.detach(), nu.detach()\n",
    "        \n",
    "        tol = 1e-6\n",
    "        max_steps = 10000\n",
    "\n",
    "        gamma = Gamma_Hellinger2.init_gamma(f, nu)\n",
    "        for i in range(max_steps):\n",
    "            gamma_prev = gamma\n",
    "            F = Gamma_Hellinger2.F(f, nu, gamma)\n",
    "            F_derivative = Gamma_Hellinger2.F_derivative(f, nu, gamma)\n",
    "            gamma = gamma_prev - F / F_derivative\n",
    "            if torch.abs(gamma - gamma_prev) < tol:\n",
    "                break\n",
    "        if torch.abs(gamma - gamma_prev) >= tol:\n",
    "            print(f\"Newton: tolerance not reached\")\n",
    "        \n",
    "        ctx.save_for_backward(f, nu, gamma)\n",
    "\n",
    "        return gamma\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        with torch.no_grad():\n",
    "            f, nu, gamma = ctx.saved_tensors\n",
    "            grad_input_f = -Gamma_Hellinger2.F_gradient(f, nu, gamma) / Gamma_Hellinger2.F_derivative(f, nu, gamma)\n",
    "        return grad_input_f * grad_output, None\n",
    "    \n",
    "    @staticmethod\n",
    "    def conjugate(f, nu):\n",
    "        gamma = Gamma_Hellinger2.apply(f, nu)\n",
    "        return torch.sum(nu * Gamma_Hellinger2.phi_conjugate(f - gamma), dim=-1) + gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hellinger2 exact: 0.32209\n"
     ]
    }
   ],
   "source": [
    "print(f\"Hellinger2 exact: {torch.sum(nu * Gamma_Hellinger2.phi(mu / nu)).item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ab8ffd79584804ab0423d301e53bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=3000.0), HTML(value='')), layout=Layout(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hellinger2 approx: 0.32209\n"
     ]
    }
   ],
   "source": [
    "iterations = 3000\n",
    "f = torch.zeros(dim, requires_grad=True)\n",
    "optimizer = torch.optim.SGD((f,), lr=1e-1, momentum=0.9)\n",
    "t = tqdm(range(iterations), ncols='100%')\n",
    "for _ in t:\n",
    "    optimizer.zero_grad()\n",
    "    objective = torch.sum(mu * f, dim=-1) - Gamma_Hellinger2.conjugate(f, nu)\n",
    "    loss = -objective\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    t.set_description(f\"objective: {objective.item():.5f}\")\n",
    "\n",
    "print(f\"Hellinger2 approx: {(torch.sum(mu * f, dim=-1) - Gamma_Hellinger2.conjugate(f, nu)).item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gamma_JS(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def init_gamma(f, nu):\n",
    "        return torch.max(f) - torch.log(torch.Tensor([2.0])) + 0.00001\n",
    "        \n",
    "    @staticmethod\n",
    "    def F(f, nu, gamma):\n",
    "        return -torch.sum(nu * (1 / (2 * torch.exp(gamma - f) - 1)), dim=-1) + 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def F_derivative(f, nu, gamma):\n",
    "        return torch.sum(nu * ((2 * torch.exp(f - gamma)) / ((torch.exp(f - gamma) - 2) ** 2)), dim=-1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def F_gradient(f, nu, gamma):\n",
    "        return -nu * ((2 * torch.exp(f - gamma)) / ((torch.exp(f - gamma) - 2) ** 2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def phi(x):\n",
    "        return x * torch.log(x) - (x + 1) * torch.log((x + 1) / 2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def phi_conjugate(y):\n",
    "        return -torch.log(2 - torch.exp(y))\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, f, nu):\n",
    "        f, nu = f.detach(), nu.detach()\n",
    "        \n",
    "        tol = 1e-6\n",
    "        max_steps = 10000\n",
    "\n",
    "        gamma = Gamma_JS.init_gamma(f, nu)\n",
    "        for i in range(max_steps):\n",
    "            gamma_prev = gamma\n",
    "            F = Gamma_JS.F(f, nu, gamma)\n",
    "            F_derivative = Gamma_JS.F_derivative(f, nu, gamma)\n",
    "            gamma = gamma_prev - F / F_derivative\n",
    "            if torch.abs(gamma - gamma_prev) < tol:\n",
    "                break\n",
    "        if torch.abs(gamma - gamma_prev) >= tol:\n",
    "            print(f\"Newton: tolerance not reached\")\n",
    "        \n",
    "        ctx.save_for_backward(f, nu, gamma)\n",
    "\n",
    "        return gamma\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        with torch.no_grad():\n",
    "            f, nu, gamma = ctx.saved_tensors\n",
    "            grad_input_f = -Gamma_JS.F_gradient(f, nu, gamma) / Gamma_JS.F_derivative(f, nu, gamma)\n",
    "        return grad_input_f * grad_output, None\n",
    "    \n",
    "    @staticmethod\n",
    "    def conjugate(f, nu):\n",
    "        gamma = Gamma_JS.apply(f, nu)\n",
    "        return torch.sum(nu * Gamma_JS.phi_conjugate(f - gamma), dim=-1) + gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JS exact: 0.30584\n"
     ]
    }
   ],
   "source": [
    "print(f\"JS exact: {torch.sum(nu * Gamma_JS.phi(mu / nu)).item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d60b02ecf714c4f860610433f674e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=2000.0), HTML(value='')), layout=Layout(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "JS approx: 0.30584\n"
     ]
    }
   ],
   "source": [
    "iterations = 2000\n",
    "f = torch.zeros(dim, requires_grad=True)\n",
    "optimizer = torch.optim.SGD((f,), lr=1e-1, momentum=0.9)\n",
    "t = tqdm(range(iterations), ncols='100%')\n",
    "for _ in t:\n",
    "    optimizer.zero_grad()\n",
    "    objective = torch.sum(mu * f, dim=-1) - Gamma_JS.conjugate(f, nu)\n",
    "    loss = -objective\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    t.set_description(f\"objective: {objective.item():.5f}\")\n",
    "\n",
    "print(f\"JS approx: {(torch.sum(mu * f, dim=-1) - Gamma_JS.conjugate(f, nu)).item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambertW(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, z):\n",
    "        z = z.detach()\n",
    "        \n",
    "        tol = 1e-5\n",
    "        max_steps = 10000\n",
    "        \n",
    "        w = torch.log(1 + z)\n",
    "        \n",
    "        for i in range(max_steps):\n",
    "            w_prev = w\n",
    "            w = w - (w * torch.exp(w) - z) / (torch.exp(w) + w * torch.exp(w))\n",
    "            if torch.abs(w - w_prev).max() < tol:\n",
    "                break\n",
    "        if torch.abs(w - w_prev).max() >= tol:\n",
    "            print(f\"LambertW: tolerance not reached\")\n",
    "        \n",
    "        ctx.save_for_backward(z, w)\n",
    "\n",
    "        return w\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        with torch.no_grad():\n",
    "            z, w = ctx.saved_tensors\n",
    "            grad_input_z = w / (z * (1 + w))\n",
    "        return grad_output * grad_input_z\n",
    "\n",
    "\n",
    "def lambertw(z):\n",
    "    return LambertW.apply(z)\n",
    "    \n",
    "\n",
    "class Gamma_Jeffreys(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def init_gamma(f, nu):\n",
    "        return torch.sum(nu * f)\n",
    "        \n",
    "    @staticmethod\n",
    "    def F(f, nu, gamma):\n",
    "        return -torch.sum(nu * (1 / lambertw(torch.exp(1 - f + gamma))), dim=-1) + 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def F_derivative(f, nu, gamma):\n",
    "        return torch.sum(nu * ((1 / (lambertw(torch.exp(1 - f + gamma)))) - (1 / (lambertw(torch.exp(1 - f + gamma)) + 1))), dim=-1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def F_gradient(f, nu, gamma):\n",
    "        return -nu * ((1 / (lambertw(torch.exp(1 - f + gamma)))) - (1 / (lambertw(torch.exp(1 - f + gamma)) + 1)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def phi(x):\n",
    "        return (x - 1) * torch.log(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def phi_conjugate(y):\n",
    "        return y + lambertw(torch.exp(1 - y)) + 1 / lambertw(torch.exp(1 - y)) - 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, f, nu):\n",
    "        f, nu = f.detach(), nu.detach()\n",
    "        \n",
    "        tol = 1e-6\n",
    "        max_steps = 10000\n",
    "\n",
    "        gamma = Gamma_Jeffreys.init_gamma(f, nu)\n",
    "        for i in range(max_steps):\n",
    "            gamma_prev = gamma\n",
    "            F = Gamma_Jeffreys.F(f, nu, gamma)\n",
    "            F_derivative = Gamma_Jeffreys.F_derivative(f, nu, gamma)\n",
    "            gamma = gamma_prev - F / F_derivative\n",
    "            if torch.abs(gamma - gamma_prev) < tol:\n",
    "                break\n",
    "        if torch.abs(gamma - gamma_prev) >= tol:\n",
    "            print(f\"Newton: tolerance not reached\")\n",
    "        \n",
    "        ctx.save_for_backward(f, nu, gamma)\n",
    "\n",
    "        return gamma\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        with torch.no_grad():\n",
    "            f, nu, gamma = ctx.saved_tensors\n",
    "            grad_input_f = -Gamma_Jeffreys.F_gradient(f, nu, gamma) / Gamma_Jeffreys.F_derivative(f, nu, gamma)\n",
    "        return grad_input_f * grad_output, None\n",
    "    \n",
    "    @staticmethod\n",
    "    def conjugate(f, nu):\n",
    "        gamma = Gamma_Jeffreys.apply(f, nu)\n",
    "        return torch.sum(nu * Gamma_Jeffreys.phi_conjugate(f - gamma), dim=-1) + gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jeffreys exact: 1.36596\n"
     ]
    }
   ],
   "source": [
    "print(f\"Jeffreys exact: {torch.sum(nu * Gamma_Jeffreys.phi(mu / nu)).item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5df213f8612647819e325f4522fcb7f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=30000.0), HTML(value='')), layout=Layout(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jeffreys approx: 1.36596\n"
     ]
    }
   ],
   "source": [
    "iterations = 30000\n",
    "f = torch.zeros(dim, requires_grad=True)\n",
    "optimizer = torch.optim.SGD((f,), lr=1e-1, momentum=0.9)\n",
    "t = tqdm(range(iterations), ncols='100%')\n",
    "for _ in t:\n",
    "    optimizer.zero_grad()\n",
    "    objective = torch.sum(mu * f, dim=-1) - (Gamma_Jeffreys.conjugate(f - torch.max(f), nu) + torch.max(f))\n",
    "    loss = -objective\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    t.set_description(f\"objective: {objective.item():.5f}\")\n",
    "\n",
    "print(f\"Jeffreys approx: {(torch.sum(mu * f, dim=-1) - Gamma_Jeffreys.conjugate(f, nu)).item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gamma_Triangular(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def init_gamma(f, nu):\n",
    "        return torch.max(f) - 1 + 0.00001\n",
    "        \n",
    "    @staticmethod\n",
    "    def F(f, nu, gamma):\n",
    "        return -torch.sum(nu * (\n",
    "            0 * (f - gamma < -3) +\n",
    "            (2 / ((1 - f + gamma) ** (1/2)) - 1) * (f - gamma >= -3)\n",
    "        ), dim=-1) + 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def F_derivative(f, nu, gamma):\n",
    "        return torch.sum(nu * (\n",
    "            0 * (f - gamma < -3) +\n",
    "            (1 / (((1 - f + gamma) ** (1/2)) ** 3)) * (f - gamma >= -3)\n",
    "        ), dim=-1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def F_gradient(f, nu, gamma):\n",
    "        return -nu * (\n",
    "            0 * (f - gamma < -3) +\n",
    "            (1 / (((1 - f + gamma) ** (1/2)) ** 3)) * (f - gamma >= -3)\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def phi(x):\n",
    "        return ((x - 1) ** 2) / (x + 1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def phi_conjugate(y):\n",
    "        return (\n",
    "            -1 * (y < -3) +\n",
    "            (4 - 4 * ((1 - y) ** (1/2)) - y) * (y >= -3)\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, f, nu):\n",
    "        f, nu = f.detach(), nu.detach()\n",
    "        \n",
    "        tol = 1e-6\n",
    "        max_steps = 10000\n",
    "\n",
    "        gamma = Gamma_Triangular.init_gamma(f, nu)\n",
    "        for i in range(max_steps):\n",
    "            gamma_prev = gamma\n",
    "            F = Gamma_Triangular.F(f, nu, gamma)\n",
    "            F_derivative = Gamma_Triangular.F_derivative(f, nu, gamma)\n",
    "            gamma = gamma_prev - F / F_derivative\n",
    "            if torch.abs(gamma - gamma_prev) < tol:\n",
    "                break\n",
    "        if torch.abs(gamma - gamma_prev) >= tol:\n",
    "            print(f\"Newton: tolerance not reached\")\n",
    "        \n",
    "        ctx.save_for_backward(f, nu, gamma)\n",
    "\n",
    "        return gamma\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        with torch.no_grad():\n",
    "            f, nu, gamma = ctx.saved_tensors\n",
    "            grad_input_f = -Gamma_Triangular.F_gradient(f, nu, gamma) / Gamma_Triangular.F_derivative(f, nu, gamma)\n",
    "        return grad_input_f * grad_output, None\n",
    "    \n",
    "    @staticmethod\n",
    "    def conjugate(f, nu):\n",
    "        gamma = Gamma_Triangular.apply(f, nu)\n",
    "        return torch.sum(nu * Gamma_Triangular.phi_conjugate(f - gamma), dim=-1) + gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triangular exact: 0.56001\n"
     ]
    }
   ],
   "source": [
    "print(f\"Triangular exact: {torch.sum(nu * Gamma_Triangular.phi(mu / nu)).item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18c0070d6e948d7b4e217bf06220378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=2000.0), HTML(value='')), layout=Layout(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Triangular approx: 0.56001\n"
     ]
    }
   ],
   "source": [
    "iterations = 2000\n",
    "f = torch.zeros(dim, requires_grad=True)\n",
    "optimizer = torch.optim.SGD((f,), lr=1e-1, momentum=0.9)\n",
    "t = tqdm(range(iterations), ncols='100%')\n",
    "for _ in t:\n",
    "    optimizer.zero_grad()\n",
    "    objective = torch.sum(mu * f, dim=-1) - Gamma_Triangular.conjugate(f, nu)\n",
    "    loss = -objective\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    t.set_description(f\"objective: {objective.item():.5f}\")\n",
    "\n",
    "print(f\"Triangular approx: {(torch.sum(mu * f, dim=-1) - Gamma_Triangular.conjugate(f, nu)).item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_tv(f, nu):\n",
    "    gamma = torch.max(f) - 1\n",
    "    return torch.sum(nu * (\n",
    "        -1 * (f - gamma < -1).float() +\n",
    "        (f - gamma) * (f - gamma >= -1).float()\n",
    "    )) + gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TV exact: 0.95936\n"
     ]
    }
   ],
   "source": [
    "print(f\"TV exact: {torch.sum(nu * torch.abs(mu / nu - 1)).item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c17cb09bf5c47339adfcd45040c7849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=50000.0), HTML(value='')), layout=Layout(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TV approx: 0.95933\n"
     ]
    }
   ],
   "source": [
    "iterations = 50000\n",
    "f = torch.zeros(dim, requires_grad=True)\n",
    "optimizer = torch.optim.SGD((f,), lr=1e-1, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda i: 1 - i / iterations)\n",
    "t = tqdm(range(iterations), ncols='100%')\n",
    "for _ in t:\n",
    "    optimizer.zero_grad()\n",
    "    objective = torch.sum(mu * f, dim=-1) - (conjugate_tv(f - torch.max(f), nu) + torch.max(f))\n",
    "    loss = -objective\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    t.set_description(f\"objective: {objective.item():.5f}\")\n",
    "\n",
    "print(f\"TV approx: {(torch.sum(mu * f, dim=-1) - conjugate_tv(f, nu)).item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
